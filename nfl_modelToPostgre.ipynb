{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fba7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n-----------\\nINFORMATION\\n-----------\\n\\nAuthor: Jean-Sebastien Roussy\\nDate Created: 2022-02-18\\nLast Modified: 2022-02-18\\n\\nDescription:\\n\\n    Original App.py file was too large for Heroku Hobby Tier Dyno, so the \\n    program was split in two to and model results table sent to \\n    Heroku Postgre DB.\\n    \\nModification History:\\n\\n               \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#------------------ Script to Save Model Results to Postgre ------------------#\n",
    "\n",
    "'''\n",
    "\n",
    "-----------\n",
    "INFORMATION\n",
    "-----------\n",
    "\n",
    "Author: Jean-Sebastien Roussy\n",
    "Date Created: 2022-02-18\n",
    "Last Modified: 2022-02-18\n",
    "\n",
    "Description:\n",
    "\n",
    "    Original App.py file was too large for Heroku Hobby Tier Dyno, so the \n",
    "    program was split in two to and model results table sent to \n",
    "    Heroku Postgre DB.\n",
    "    \n",
    "Modification History:\n",
    "\n",
    "               \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e471fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### Imports ###\n",
    "###############\n",
    "\n",
    "from os import getenv\n",
    "from os.path import join\n",
    "from io import BytesIO, StringIO\n",
    "from joblib import dump, load\n",
    "import boto3\n",
    "import tempfile\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import pi\n",
    "\n",
    "from sqlalchemy import Table, Column, Integer, Float, DateTime, Text, ForeignKey\n",
    "from sqlalchemy import create_engine, MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afbdc1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "### Global Variables ###\n",
    "########################\n",
    "\n",
    "dtype_pandasToSQL = {'int32': Integer,\n",
    "                     'int64': Integer,\n",
    "                     'float32': Float,\n",
    "                     'float64': Float,\n",
    "                     'object': Text,\n",
    "                     'datetime64[ns, UTC]': DateTime}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64d3d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### Functions ###\n",
    "#################\n",
    "\n",
    "#################################\n",
    "### Feature Engineering Funcs ###\n",
    "#################################\n",
    "\n",
    "# For time based columns\n",
    "def transformation(column):\n",
    "    max_value = column.max()\n",
    "    sin_values = [math.sin((2*pi*x)/max_value) for x in list(column)]\n",
    "    cos_values = [math.cos((2*pi*x)/max_value) for x in list(column)]\n",
    "    return sin_values, cos_values\n",
    "\n",
    "######################\n",
    "### Read SQL to DF ###\n",
    "######################\n",
    "\n",
    "# Source: https://towardsdatascience.com/optimizing-pandas-read-sql-for-postgres-f31cd7f707ab\n",
    "def read_sql_tmpfile(query, dbengine, table=None):\n",
    "    with tempfile.TemporaryFile() as tmpfile:\n",
    "        if table == 'matview':\n",
    "            copy_sql = 'COPY (SELECT * FROM \"{query}\") TO STDOUT WITH CSV {head}'.format(\n",
    "                        query=query, head=\"HEADER\")\n",
    "        else:\n",
    "            copy_sql = 'COPY \"{query}\" TO STDOUT WITH CSV {head}'.format(\n",
    "                        query=query, head=\"HEADER\")\n",
    "        conn = dbengine.raw_connection()\n",
    "        cur = conn.cursor()\n",
    "        cur.copy_expert(copy_sql, tmpfile)\n",
    "        tmpfile.seek(0)\n",
    "        df = pd.read_csv(tmpfile)\n",
    "        return df\n",
    "    \n",
    "#######################\n",
    "### S3 Loading Func ###\n",
    "#######################\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/62941174/how-to-write-load-machine-learning-model-to-from-s3-bucket-through-joblib\n",
    "def read_joblib(path):\n",
    "    ''' \n",
    "       Function to load a joblib file from an s3 bucket or local directory.\n",
    "       Arguments:\n",
    "       * path: an s3 bucket or local directory path where the file is stored\n",
    "       Outputs:\n",
    "       * file: Joblib file loaded\n",
    "    '''\n",
    "\n",
    "    # Path is an s3 bucket\n",
    "    if path[:5] == 's3://':\n",
    "        s3_bucket, s3_key = path.split('/')[2], path.split('/')[3:]\n",
    "        s3_key = '/'.join(s3_key)\n",
    "        with BytesIO() as f:\n",
    "            boto3.client(\"s3\").download_fileobj(Bucket=s3_bucket, Key=s3_key, Fileobj=f)\n",
    "            f.seek(0)\n",
    "            file = load(f)\n",
    "    \n",
    "    return file\n",
    "\n",
    "#####################\n",
    "### Postgre Funcs ###\n",
    "#####################\n",
    "\n",
    "# List column names and dtypes from pandas dataframe\n",
    "def pandas_data_info(df):\n",
    "    col_lst = df.columns.tolist()\n",
    "    dtype_lst = df.dtypes\n",
    "    df_info_lst = list(zip(col_lst, dtype_lst))\n",
    "    return df_info_lst\n",
    "\n",
    "# Drop table if it exists with SQLALchemy\n",
    "# Source: https://stackoverflow.com/questions/35918605/how-to-delete-a-table-in-sqlalchemy/66376565#66376565\n",
    "def drop_table(table_name, dbengine):\n",
    "    metadata = MetaData()\n",
    "    metadata.reflect(bind=dbengine)\n",
    "    with dbengine.connect() as conn:\n",
    "        if table_name in metadata.tables:\n",
    "            table = metadata.tables[table_name]\n",
    "            print(f'Table {table_name} to be removed.')\n",
    "            metadata.drop_all(conn, [table], checkfirst=True)\n",
    "            print(f'Table {table_name} deleted!')\n",
    "        else:\n",
    "            print(f'Table {table_name} does not exist.')\n",
    "            \n",
    "# Create single table with SQLAlchemy\n",
    "def create_table(dbengine, table_name, column_info):\n",
    "    metadata = MetaData()\n",
    "    with dbengine.connect() as conn:\n",
    "        # Set new table and create it\n",
    "        table = Table(name,\n",
    "                      metadata,\n",
    "                      #Column('id', Integer, primary_key=True),\n",
    "                      *(Column(name, dtype) for name, dtype in column_info if name != 'id'))\n",
    "        print(f'Table {name} created!')\n",
    "        # Create all tables\n",
    "        metadata.create_all(conn, checkfirst=False)\n",
    "        return table\n",
    "\n",
    "# Insert table values\n",
    "# Alternative to_sql() *method* for DBs that support COPY FROM\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-sql-method\n",
    "# https://stackoverflow.com/questions/23103962/how-to-write-dataframe-to-postgres-table\n",
    "def psql_insert_copy(table, engine, keys, data_iter):\n",
    "    \"\"\"\n",
    "    Execute SQL statement inserting data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : pandas.io.sql.SQLTable\n",
    "    conn  : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n",
    "    keys  : list of str\n",
    "            Column names\n",
    "    data_iter : Iterable that iterates the values to be inserted\n",
    "    \"\"\"\n",
    "    # gets a DBAPI connection that can provide a cursor\n",
    "    dbapi_conn = engine.connection\n",
    "    with dbapi_conn.cursor() as cur:\n",
    "        s_buf = StringIO()\n",
    "        writer = csv.writer(s_buf)\n",
    "        writer.writerows(data_iter)\n",
    "        s_buf.seek(0)\n",
    "\n",
    "        columns = ', '.join(['\"{}\"'.format(k) for k in keys])\n",
    "        if table.schema:\n",
    "            table_name = '{}.\"{}\"'.format(table.name) # Case sensitive table names require double quotes\n",
    "        else:\n",
    "            table_name = table.name\n",
    "\n",
    "        sql = 'COPY \"{}\" ({}) FROM STDIN WITH CSV'.format(\n",
    "            table_name, columns)\n",
    "        cur.copy_expert(sql=sql, file=s_buf)\n",
    "        print(f'Data succesfully copied into {table_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19f844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "### POSTGRESQL INFO ###\n",
    "#######################\n",
    "\n",
    "# Load in environment file\n",
    "load_dotenv()\n",
    "\n",
    "# SQLAlchemy engine from env file\n",
    "db_uri = getenv('SQLALCHEMY_NFL_URI')\n",
    "if db_uri.startswith(\"postgres://\"):\n",
    "    db_uri = db_uri.replace(\"postgres://\", \"postgresql+psycopg2://\", 1)\n",
    "\n",
    "engine = create_engine(db_uri, \n",
    "                       convert_unicode=True, \n",
    "                       encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d56b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### LOAD DATA ###\n",
    "#################\n",
    "\n",
    "# Load Statistics\n",
    "team_stats_records_df = read_sql_tmpfile('team_stats_records', engine, table='matview')\n",
    "# Load matches\n",
    "matches_df = read_sql_tmpfile('matches', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c819a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "### JOIN DATA & CLEAN ###\n",
    "#########################\n",
    "\n",
    "# Merge Remerged df to Matches df\n",
    "nfl_df = matches_df.merge(team_stats_records_df, left_on=['season', 'home_team'], \n",
    "                          right_on=['season','id'])\\\n",
    "                   .merge(team_stats_records_df, left_on=['season', 'away_team'], \n",
    "                          right_on=['season','id'],\n",
    "                          suffixes=('_home', '_away'))\n",
    "\n",
    "# Sort NFL df by date_time\n",
    "nfl_df = nfl_df.sort_values(['date_time'])\n",
    "\n",
    "# Drop redundant columns like id_x/id_y, etc\n",
    "drop_cols = [col for col in nfl_df.columns if '_x' in col or '_y' in col]\n",
    "nfl_df = nfl_df.drop(drop_cols, axis=1)\n",
    "\n",
    "# Drop stats columns with 999 values in it\n",
    "drop_999_cols = [col for col in nfl_df.columns[nfl_df.isin([999]).any()]\\\n",
    "                   if 'stat' in col]\n",
    "nfl_df = nfl_df.drop(drop_999_cols, axis=1)\n",
    "\n",
    "# Drop stats and record columns with only 0 values\n",
    "drop_0_cols = [col for col, is_zero in ((nfl_df == 0).sum() == nfl_df.shape[0])\\\n",
    "               .items() if is_zero and ('stat' in col or 'record' in col)]\n",
    "nfl_df = nfl_df.drop(drop_0_cols, axis=1)\n",
    "\n",
    "# Replace 999 values in matches info\n",
    "replace_999_cols = ['home_team_win', 'home_team_score', \n",
    "                    'away_team_win', 'away_team_score']\n",
    "nfl_df[replace_999_cols] = nfl_df[replace_999_cols].replace(999,0)\n",
    "\n",
    "# Replace remaining NaNs with 0\n",
    "nfl_df = nfl_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b2fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### FEATURE ENGINEERING ###\n",
    "###########################\n",
    "\n",
    "# Date_time to Month, Day of Week, Time of Day as ints\n",
    "nfl_df['date_time'] = pd.to_datetime(nfl_df['date_time'])\n",
    "nfl_df['month'] = nfl_df['date_time'].dt.month.astype('int64')\n",
    "nfl_df['day_week'] = nfl_df['date_time'].dt.dayofweek.astype('int64')\n",
    "nfl_df['time_day'] = nfl_df['date_time'].dt.hour.astype('int64')\n",
    "\n",
    "# Calculate Cosine and Sine for new dates\n",
    "for col in ['month', 'day_week', 'time_day']:\n",
    "    time_sine, time_cos = transformation(nfl_df[col])\n",
    "    nfl_df[col[0:3]+'_sine'] = time_sine\n",
    "    nfl_df[col[0:3]+'_cos'] = time_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb18cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "### X & y SPLIT ###\n",
    "###################\n",
    "\n",
    "# X cols list\n",
    "keep_cols = ['stat', 'record', 'sine', 'cos',\n",
    "             'division', 'conference', 'team_name', 'location']\n",
    "\n",
    "remove_cols = ['team_win', 'team_score', 'id', 'date_time',\n",
    "               'month', 'day_week', 'time_day']\n",
    "\n",
    "X_cols =  [col for col in list(nfl_df.columns)\\\n",
    "                    if any(kc in col for kc in keep_cols)\\\n",
    "           and any(rc not in col for rc in remove_cols)]\n",
    "\n",
    "## Split X and y into Train and Test\n",
    "# X,y Train\n",
    "X_train = nfl_df[~nfl_df['season'].isin([2021])][X_cols]\n",
    "y_train = nfl_df[~nfl_df['season'].isin([2021])]['home_team_win']\n",
    "y_regtrain = nfl_df[~nfl_df['season'].isin([2021])][['home_team_score',\n",
    "                                                     'away_team_score']]\n",
    "# X\n",
    "X_test = nfl_df[nfl_df['season'].isin([2021])][X_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de72e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "### LOAD MODEL ###\n",
    "##################\n",
    "\n",
    "# Load Logistic Regresssion Model\n",
    "xgb_model = read_joblib(getenv('XGB_URI'))\n",
    "# Load SVR Model\n",
    "svr_model = read_joblib(getenv('SVR_URI'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f81674b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jean\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:53:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"num_round\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jean\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n",
      "C:\\Users\\Jean\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:255: ConvergenceWarning: Solver terminated early (max_iter=500).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### 2021 PREDICTIONS ###\n",
    "########################\n",
    "\n",
    "## 2021 DATA \n",
    "\n",
    "# Fit Model Logistic Regression\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels\n",
    "pred_label = xgb_model.predict(X_test)\n",
    "\n",
    "# Probabilities\n",
    "prob = xgb_model.predict_proba(X_test)\n",
    "\n",
    "# Fit Model SVR\n",
    "svr_model.fit(X_train, y_regtrain)\n",
    "\n",
    "# Predict Scores\n",
    "pred_scores = svr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c19204",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "### 2021 TABLES ###\n",
    "###################\n",
    "\n",
    "# Weekly results Table\n",
    "nfl_df2021 = nfl_df[nfl_df['season'] == 2021][['team_name_home',\n",
    "                                               'home_team_score',\n",
    "                                               'away_team_score',\n",
    "                                               'team_name_away',\n",
    "                                               'week',\n",
    "                                               'home_team_win']]\n",
    "\n",
    "# Change column names\n",
    "nfl_df2021 = nfl_df2021.rename(columns={'team_name_home':'home_team',\n",
    "                                        'team_name_away':'away_team',\n",
    "                                        'home_team_score':'home_score',\n",
    "                                        'away_team_score':'away_score'})\n",
    "\n",
    "# Add probability column to df\n",
    "nfl_df2021['home_team_win_probability'] = [int(round(x*100)) for x in prob[:,1]]\n",
    "# Add predicted labels\n",
    "nfl_df2021['predicted_winner'] = pred_label\n",
    "# Add predicted Home Score\n",
    "nfl_df2021['predicted_home_score'] = [int(round(x)) for x in pred_scores[:,0]]\n",
    "# Add predicted Home Score\n",
    "nfl_df2021['predicted_away_score'] = [int(round(x)) for x in pred_scores[:,1]]\n",
    "\n",
    "# Map actual winner name to column\n",
    "nfl_df2021['actual_winner'] = nfl_df2021.apply(lambda x: x.home_team if x.home_team_win == 1 else x.away_team, axis=1)\n",
    "# Map predicted winner name to column\n",
    "nfl_df2021['predicted_winner'] = nfl_df2021.apply(lambda x: x.home_team if x.predicted_winner == 1 else x.away_team, axis=1)\n",
    "# Map win probability\n",
    "nfl_df2021['win_probability'] = nfl_df2021.apply(lambda x: x.home_team_win_probability if x.predicted_winner == x.home_team else 100 - x.home_team_win_probability, axis=1)\n",
    "# Map correct predicition\n",
    "nfl_df2021['correct_prediction'] = (nfl_df2021['predicted_winner'] == nfl_df2021['actual_winner']).astype(int)\n",
    "# Replace Actual Winner value with None if game not played yet\n",
    "nfl_df2021['actual_winner'] = np.where((nfl_df2021['home_score'] == 0) & (nfl_df2021['away_score'] == 0), \"None\", nfl_df2021['actual_winner'])\n",
    "# Replace Correct Prediction value with -1 if game not played yet\n",
    "nfl_df2021['correct_prediction'] = np.where((nfl_df2021['home_score'] == 0) & (nfl_df2021['away_score'] == 0), -1, nfl_df2021['correct_prediction'])\n",
    "\n",
    "# Drop columns\n",
    "nfl_df2021 = nfl_df2021.drop(columns=['home_team_win_probability', 'home_team_win'])\n",
    "\n",
    "# Move Cols\n",
    "move_cols = nfl_df2021.columns.to_list()\n",
    "new_order = move_cols[4:5] + move_cols[0:2] + move_cols[6:8] + move_cols[2:4] + move_cols[5:6] + move_cols[8:]\n",
    "nfl_df2021 = nfl_df2021[new_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "763c3a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('week', <class 'sqlalchemy.sql.sqltypes.Integer'>), ('home_team', <class 'sqlalchemy.sql.sqltypes.Text'>), ('home_score', <class 'sqlalchemy.sql.sqltypes.Integer'>), ('predicted_home_score', <class 'sqlalchemy.sql.sqltypes.Integer'>), ('predicted_away_score', <class 'sqlalchemy.sql.sqltypes.Integer'>), ('away_score', <class 'sqlalchemy.sql.sqltypes.Integer'>), ('away_team', <class 'sqlalchemy.sql.sqltypes.Text'>), ('predicted_winner', <class 'sqlalchemy.sql.sqltypes.Text'>), ('actual_winner', <class 'sqlalchemy.sql.sqltypes.Text'>), ('win_probability', <class 'sqlalchemy.sql.sqltypes.Integer'>), ('correct_prediction', <class 'sqlalchemy.sql.sqltypes.Integer'>)]\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "### DATA INFO ###\n",
    "#################\n",
    "\n",
    "# Grab column names\n",
    "# Add DF col/dtype info to var\n",
    "col_info = pandas_data_info(nfl_df2021)\n",
    "    \n",
    "# Convert dtype to match PostgreSQL dtypes and add to empty list\n",
    "ncol_type_lst = []\n",
    "for col, dtype in col_info:\n",
    "    if dtype_pandasToSQL[str(dtype)]:\n",
    "        dtype = dtype_pandasToSQL[str(dtype)]\n",
    "        ncol_type_lst.append((col, dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cd5dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table nfl_2021_season to be removed.\n",
      "Table nfl_2021_season deleted!\n",
      "Table nfl_2021_season created!\n"
     ]
    }
   ],
   "source": [
    "#####################\n",
    "### CREATE TABLES ###\n",
    "#####################\n",
    "\n",
    "# NFL DF table name\n",
    "name = 'nfl_2021_season'\n",
    "\n",
    "# Delete table if exists\n",
    "drop_table(name, engine)\n",
    "\n",
    "# Create table\n",
    "nfl_table = create_table(engine, name, ncol_type_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29875b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data succesfully copied into nfl_2021_season\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "### INSERT DATA ###\n",
    "###################\n",
    "\n",
    "# Add teams table data first\n",
    "nfl_df2021.to_sql(name, engine, if_exists='append', index=False, method=psql_insert_copy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cdbcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
