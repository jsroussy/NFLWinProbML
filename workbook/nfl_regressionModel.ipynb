{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe4f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "### Imports ###\n",
    "###############\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from math import pi\n",
    "from joblib import dump\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7414dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### Functions ###\n",
    "#################\n",
    "\n",
    "###############################\n",
    "### File Manipulation Funcs ###\n",
    "###############################\n",
    "\n",
    "# File listing function\n",
    "def zlist_files(data_path, zipname, cond):\n",
    "    zipped_files = join(data_path, zipname)\n",
    "    with ZipFile(zipped_files) as z:\n",
    "        flist = [f for f in z.namelist() if cond in f]\n",
    "        fout = [z.open(f) for f in flist]\n",
    "        return fout\n",
    "\n",
    "# File listing function\n",
    "def list_files(data_path, cond):\n",
    "    flist = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "    fcond = [join(data_path, f) for f in flist if cond in f]\n",
    "    return fcond\n",
    "\n",
    "# Create pandas data loader for multi-json loads\n",
    "def pandas_loader(flist, idx_name):\n",
    "    dfs = [pd.read_json(f) for f in flist]\n",
    "    df = pd.concat((idf.set_index(idx_name) for idf in dfs), \n",
    "                   axis=1, join='inner').reset_index()\n",
    "    return df\n",
    "\n",
    "#################################\n",
    "### Feature Engineering Funcs ###\n",
    "#################################\n",
    "\n",
    "# For time based columns\n",
    "def transformation(column):\n",
    "    max_value = column.max()\n",
    "    sin_values = [math.sin((2*pi*x)/max_value) for x in list(column)]\n",
    "    cos_values = [math.cos((2*pi*x)/max_value) for x in list(column)]\n",
    "    return sin_values, cos_values\n",
    "\n",
    "###################\n",
    "### Stats Funcs ###\n",
    "###################\n",
    "\n",
    "# Verify column skew for normalization requirement\n",
    "def skewness_test(df, col_list):\n",
    "    x_var = df[col_list]\n",
    "    skew_data = pd.DataFrame()\n",
    "    skew_data['features'] = x_var.columns\n",
    "    skew_data['skew'] = x_var.skew().values\n",
    "    return skew_data\n",
    "\n",
    "# Verify Column Vif scores for multicollinearity \n",
    "def vif_test(df, col_list):\n",
    "    x_var = df[col_list]\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['features'] = x_var.columns\n",
    "    vif_data['vif'] = [variance_inflation_factor(x_var.values, i)\\\n",
    "                      for i in range(len(x_var.columns))]\n",
    "    return vif_data\n",
    "\n",
    "##################################\n",
    "### Pipeline & Modelling Funcs ###\n",
    "##################################\n",
    "# Pipeline model with Gridsearch\n",
    "def pipe_grid_model(xtrain, ytrain,\n",
    "                    numeric_features, categorical_features,\n",
    "                    estimator='LinearRegression'):\n",
    "    \n",
    "    # Numerical features pipeline with StandardScaler\n",
    "    numeric_transformer = Pipeline([('scaler', StandardScaler()),\n",
    "                                    ('pca', PCA())])\n",
    "    # Categorical features pipeline with OHE\n",
    "    categorical_transformer = OneHotEncoder(drop='first')\n",
    "    \n",
    "    # Combine Num and Cat feature Pipelines into preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer, numeric_features),\n",
    "                     ('cat', categorical_transformer, categorical_features)])\n",
    "    \n",
    "    if estimator == 'LinearRegression':\n",
    "        # Set pipeline with Preprocessor, Regressor\n",
    "        pipe = Pipeline([('preprocessor', preprocessor),\n",
    "                         ('reg', LinearRegression())])\n",
    "\n",
    "        # Parametrs grid\n",
    "        param_grid = [{'preprocessor__num__pca__n_components': np.arange(50,500,50)}]\n",
    "\n",
    "    if estimator == 'SVR':\n",
    "        # Set pipeline with Scaler, Regressor\n",
    "        pipe = Pipeline([('preprocessor', preprocessor),\n",
    "                         ('svr', MultiOutputRegressor(SVR()))])\n",
    "\n",
    "        # Parametrs grid\n",
    "        param_grid = [{'preprocessor__num__pca__n_components': np.arange(50,500,50),\n",
    "                       'svr__estimator__kernel': ['linear', 'poly', 'rbf'],\n",
    "                       'svr__estimator__degree': [2,3],\n",
    "                       'svr__estimator__gamma': ['scale', 'auto'],\n",
    "                       'svr__estimator__coef0': [0.0, 0.01, 0.1],\n",
    "                       'svr__estimator__tol': [0.0001, 0.001, 0.01],\n",
    "                       'svr__estimator__C': [0.1, 1, 3],\n",
    "                       'svr__estimator__epsilon': [0.01, 0.1, 1],\n",
    "                       'svr__estimator__max_iter': [100,500,1000]}]\n",
    "\n",
    "    # Call grid function with parameters and pipeline\n",
    "    grid = HalvingRandomSearchCV(pipe, param_grid, cv=10)\n",
    "    grid.fit(xtrain, ytrain)\n",
    "        \n",
    "    # Review best params\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    # Create model\n",
    "    model = grid.best_estimator_\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Pipeline transform X Train and Test\n",
    "def pipe_transform(xtrain, xtest,\n",
    "                  numeric_features, categorical_features):\n",
    "    \n",
    "    # Numerical features pipeline with StandardScaler\n",
    "    numeric_transformer = Pipeline([('scaler', StandardScaler())])\n",
    "    # Categorical features pipeline with OHE\n",
    "    categorical_transformer = OneHotEncoder(drop='first')\n",
    "    \n",
    "    # Combine Num and Cat feature Pipelines into preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', numeric_transformer, numeric_features),\n",
    "                     ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "    # Set pipeline with Scaler, Regressor\n",
    "    pipe = Pipeline([('preprocessor', preprocessor)])\n",
    "\n",
    "    # Fit transform X\n",
    "    pipe.fit(xtrain)\n",
    "    \n",
    "    # Transform Test\n",
    "    xtrainTrans = pipe.transform(xtrain)\n",
    "    xtestTrans = pipe.transform(xtest)\n",
    "    \n",
    "    return xtrainTrans, xtestTrans\n",
    "\n",
    "## Iteratively run model until all predictors in model are significant\n",
    "## Source: https://towardsdatascience.com/feature-selection-with-pandas-e3690ad8504b\n",
    "# Create model using backward elimination\n",
    "def backward_elimination(x, y, alpha=0.05):\n",
    "    col_idx = [i for i in range(x.shape[1])]\n",
    "    x = np.r_[[col_idx], x]\n",
    "    removed_cols = []\n",
    "    while x.shape[1]>0:\n",
    "        p = []\n",
    "        x1 = x[1:,:]\n",
    "        # Add constant\n",
    "        x1 = sm.add_constant(x1)\n",
    "        # regression model\n",
    "        result = sm.OLS(y, x1).fit()\n",
    "        # Create list of pvalues with cols as index\n",
    "        p = result.pvalues[1:].to_numpy()\n",
    "        # Obtain the maximum pvalue in list\n",
    "        pmax = max(p)\n",
    "        # Obtain the index of the maximum pvalue\n",
    "        feature_with_pmax = np.where(p == pmax)[0][0]\n",
    "        # Verify if the pmax is less than 0.05 if not remove column from list\n",
    "        # Run until the all values are less than 0.05\n",
    "        if pmax > alpha:\n",
    "            removed_cols.append(x[:1][0][feature_with_pmax])\n",
    "            x = np.delete(x, feature_with_pmax, axis=1)\n",
    "        else:\n",
    "            break \n",
    "    return removed_cols, x1[:,1:], result\n",
    "\n",
    "## Check for multicollinearity \n",
    "## Source: https://www.datasklr.com/ols-least-squares-regression/multicollinearity\n",
    "# Remove cols with over x VIF score\n",
    "def vif_elimination(x, vfactor=5):\n",
    "    col_idx = [i for i in range(x.shape[1])]\n",
    "    x = np.r_[[col_idx], x]\n",
    "    removed_cols = []\n",
    "    while x.shape[1]>0:\n",
    "        v = []\n",
    "        x1 = x[1:,:]\n",
    "        # VIF dataframe\n",
    "        # Calculating VIF for each feature\n",
    "        v = [variance_inflation_factor(x1, i) for i in range(x1.shape[1])]\n",
    "        # Obtain the maximum vif value in list\n",
    "        vmax = max(v)\n",
    "        # Obtain the index of the maximum vif value\n",
    "        feature_with_vmax = np.where(v == vmax)[0][0]\n",
    "        # Verify if the vmax is less than 1 if not remove column from list\n",
    "        # Run until the all values are less than 1, or not correlated\n",
    "        if vmax > vfactor:\n",
    "            removed_cols.append(x[:1][0][feature_with_vmax])\n",
    "            x = np.delete(x, feature_with_vmax, axis=1)\n",
    "        else:\n",
    "            break\n",
    "    return removed_cols, x1\n",
    "\n",
    "# OLS VIF Pipeline\n",
    "def ols_vif_regressor(xtrain, ytrain, alphas=[0.3,0.1], vfactor=10):\n",
    "    # Run OLS backwards elimination model\n",
    "    xr_ols, x_ols, res = backward_elimination(xtrain, ytrain, alpha=alphas[0])\n",
    "    # Run VIF removal\n",
    "    xr_vif, x_vif = vif_elimination(x_ols, vfactor=vfactor)\n",
    "    # Run OLS backwards elimination model again\n",
    "    xr_ols2, x_ols2, res2 = backward_elimination(x_vif, ytrain, alpha=alphas[1])\n",
    "\n",
    "    return xr_ols, xr_vif, xr_ols2, res2\n",
    "\n",
    "# Predict scores\n",
    "def predict_score(xtest, xr_ols, xr_vif, xr_ols2, res):\n",
    "    # Predict Scores\n",
    "    x_rc = np.delete(xtest, xr_ols, axis=1)\n",
    "    x_rc1 = np.delete(x_rc, xr_vif, axis=1)\n",
    "    x_rc2 = np.delete(x_rc1, xr_ols2, axis=1)\n",
    "    x_rc3 = sm.add_constant(x_rc2, has_constant='add')\n",
    "    pred_score = res.predict(x_rc3)\n",
    "    \n",
    "    return pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6479a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### LOAD DATA ###\n",
    "#################\n",
    "\n",
    "data_path = '../data'\n",
    "\n",
    "# Load Statistics\n",
    "stats_df = pandas_loader(zlist_files(data_path, 'nfl_data_2002_2021.zip', 'stats'), 'id')\n",
    "#stats_df.head(10)\n",
    "\n",
    "# Load Record\n",
    "record_df = pandas_loader(zlist_files(data_path, 'nfl_data_2002_2021.zip', 'record'), 'id')\n",
    "#record_df.head(10)\n",
    "\n",
    "# Load matches\n",
    "matches_df = pd.read_json(zlist_files(data_path, 'nfl_data_2002_2021.zip', 'matches')[0])\n",
    "#matches_df.head(10)\n",
    "\n",
    "# Load teams\n",
    "teams_df = pd.read_json(zlist_files(data_path, 'nfl_data_2002_2021.zip', 'teams')[0])\n",
    "#teams_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb159ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "### JOIN DATA & CLEAN ###\n",
    "#########################\n",
    "\n",
    "# Remove Duplicate columns from Stats and Records df\n",
    "stats_df = stats_df.loc[:,~stats_df.columns.duplicated()]\n",
    "record_df = record_df.loc[:,~record_df.columns.duplicated()]\n",
    "\n",
    "# Merge Stats and Record dfs\n",
    "merged_df = stats_df.merge(record_df, how='left', on=['id'])\n",
    "#merged_df.head(10)\n",
    "\n",
    "# Merge Merged df with Teams df\n",
    "remerge_df = merged_df.merge(teams_df, how='left', left_on='team_id_x', \n",
    "                             right_on='id')\n",
    "#remerge_df.head(35)\n",
    "\n",
    "# Merge Remerged df to Matches df\n",
    "nfl_df = matches_df.merge(remerge_df, left_on=['season', 'home_team'], \n",
    "                          right_on=['season_x','team_id_x'])\\\n",
    "                   .merge(remerge_df, left_on=['season', 'away_team'], \n",
    "                          right_on=['season_x','team_id_x'],\n",
    "                          suffixes=('_home', '_away'))\n",
    "\n",
    "# Convert date_time column to datetime object\n",
    "nfl_df['date_time'] = pd.to_datetime(nfl_df['date_time'], utc=True)\n",
    "# Sort NFL df by date_time\n",
    "nfl_df = nfl_df.sort_values(['date_time'])\n",
    "\n",
    "# Drop redundant columns like id_x/id_y, etc\n",
    "drop_cols = [col for col in nfl_df.columns if '_x_' in col or '_y_' in col]\n",
    "nfl_df = nfl_df.drop(drop_cols, axis=1)\n",
    "\n",
    "# Drop Rank and PG columns\n",
    "drop_rankpg_cols = [col for col in nfl_df.columns\\\n",
    "                    if 'rank' in col or 'pg' in col]\n",
    "nfl_df = nfl_df.drop(drop_rankpg_cols, axis=1)\n",
    "\n",
    "# Drop stats columns with only 999 values in it\n",
    "drop_999_cols = [col for col in nfl_df.columns[nfl_df.isin([999]).any()]\\\n",
    "                   if 'stat' in col]\n",
    "nfl_df = nfl_df.drop(drop_999_cols, axis=1)\n",
    "\n",
    "# Drop stats and record columns with only 0 values\n",
    "drop_0_cols = [col for col, is_zero in ((nfl_df == 0).sum() == nfl_df.shape[0])\\\n",
    "               .items() if is_zero and ('stat' in col or 'record' in col)]\n",
    "nfl_df = nfl_df.drop(drop_0_cols, axis=1)\n",
    "\n",
    "# Replace 999 values in matches info\n",
    "replace_999_cols = ['home_team_win', 'home_team_score', \n",
    "                    'away_team_win', 'away_team_score']\n",
    "nfl_df[replace_999_cols] = nfl_df[replace_999_cols].replace(999,0)\n",
    "\n",
    "# Drop Abbreviation team name col\n",
    "drop_abb_col = [col for col in nfl_df.columns if 'abbreviation' in col]\n",
    "nfl_df = nfl_df.drop(drop_abb_col, axis=1)\n",
    "\n",
    "# Replace remaining NaNs with 0\n",
    "nfl_df = nfl_df.fillna(0)\n",
    "\n",
    "nfl_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc01d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "### DATA VERIFICATION ###\n",
    "#########################\n",
    "\n",
    "# Float and Int cols with Stats and Record\n",
    "verification_cols = [col for col in list(nfl_df.select_dtypes\\\n",
    "                                         (include=['float64', 'int64']).columns)\\\n",
    "                    if 'stat' in col or 'record' in col]\n",
    "\n",
    "# Skewness of the data for int and float columns\n",
    "skew_data = skewness_test(nfl_df, verification_cols)\n",
    "print(skew_data[(skew_data['skew'] > 0.5) | (skew_data['skew'] < -0.5)]\\\n",
    "      .sort_values('skew', ascending=False))\n",
    "skew_data['skew'].hist(bins='auto')\n",
    "plt.show()\n",
    "        \n",
    "# View Columns with high multicollinearity\n",
    "vif_data = vif_test(nfl_df, verification_cols)\n",
    "print(vif_data[vif_data['vif'] > 1].sort_values('vif', ascending=False))\n",
    "vif_data[vif_data['vif'] < 1000]['vif'].hist(bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03af1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### FEATURE ENGINEERING ###\n",
    "###########################\n",
    "\n",
    "# Date_time to Month, Day of Week, Time of Day as ints\n",
    "nfl_df['month'] = nfl_df['date_time'].dt.month.astype('int64')\n",
    "nfl_df['day_week'] = nfl_df['date_time'].dt.dayofweek.astype('int64')\n",
    "nfl_df['time_day'] = nfl_df['date_time'].dt.hour.astype('int64')\n",
    "\n",
    "# Calculate Cosine and Sine for new dates\n",
    "for col in ['month', 'day_week', 'time_day']:\n",
    "    time_sine, time_cos = transformation(nfl_df[col])\n",
    "    nfl_df[col[0:3]+'_sine'] = time_sine\n",
    "    nfl_df[col[0:3]+'_cos'] = time_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e849e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "### X & y SPLIT ###\n",
    "###################\n",
    "\n",
    "# X cols list\n",
    "keep_cols = ['stat', 'record', 'sine', 'cos',\n",
    "             'division', 'conference', 'team_name', 'location']\n",
    "\n",
    "remove_cols = ['team_win', 'team_Score', 'id', 'date_time',\n",
    "               'month', 'day_week', 'time_day']\n",
    "\n",
    "X_cols =  [col for col in list(nfl_df.columns)\\\n",
    "                    if any(kc in col for kc in keep_cols)\\\n",
    "           and any(rc not in col for rc in remove_cols)]\n",
    "\n",
    "## Split X and y into Train and Test\n",
    "# X,y Train\n",
    "X_train = nfl_df[~nfl_df['season'].isin([2021])][X_cols]\n",
    "y_train = nfl_df[~nfl_df['season'].isin([2021])][['home_team_score',\n",
    "                                                  'away_team_score']]\n",
    "y_htrain = nfl_df[~nfl_df['season'].isin([2021])]['home_team_score']\n",
    "y_atrain = nfl_df[~nfl_df['season'].isin([2021])]['away_team_score']\n",
    "# X,y Test\n",
    "X_test = nfl_df[nfl_df['season'].isin([2021])][X_cols]\n",
    "y_test = nfl_df[~nfl_df['season'].isin([2021])][['home_team_score',\n",
    "                                                 'away_team_score']]\n",
    "y_htest = nfl_df[nfl_df['season'].isin([2021])]['home_team_score']\n",
    "y_atest = nfl_df[nfl_df['season'].isin([2021])]['away_team_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### MODELLING: SM LINEAR REGRESSION ###\n",
    "#######################################\n",
    "\n",
    "# Numerical cols to StandardScaler\n",
    "num_cols = [col for col in list(X_train.select_dtypes\\\n",
    "                                         (include=['float64', 'int64']).columns)\\\n",
    "                    if 'stat' in col or 'record' in col]\n",
    "\n",
    "# Categorical cols to OHE\n",
    "cat_cols = ['division_home', 'conference_home', \n",
    "            'location_home', 'team_name_home',\n",
    "            'division_away', 'conference_away', \n",
    "            'location_away', 'team_name_away']\n",
    "\n",
    "# Process and Transform X_train and X_test\n",
    "X_trainTrans, X_testTrans = pipe_transform(X_train, X_test,\n",
    "                                           num_cols, cat_cols)\n",
    "\n",
    "# Run OLS Vif Pipeline on Home Games\n",
    "Xr_hols, Xr_hvif, Xr_hols1, hres = ols_vif_regressor(X_trainTrans, \n",
    "                                                     y_htrain, \n",
    "                                                     alphas=[0.3,0.1], \n",
    "                                                     vfactor=10)\n",
    "# Run OLS Vif Pipeline on Away Games\n",
    "Xr_aols, Xr_avif, Xr_aols1, ares = ols_vif_regressor(X_trainTrans, \n",
    "                                                     y_atrain, \n",
    "                                                     alphas=[0.3,0.1], \n",
    "                                                     vfactor=10)\n",
    "\n",
    "print(hres.summary())\n",
    "print(ares.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397e3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "### SAVE MODELS ###\n",
    "###################\n",
    "\n",
    "# Model path\n",
    "model_path = '../model'\n",
    "\n",
    "# file names\n",
    "home_model = 'homescore_ols_2002_2020.pickle'\n",
    "away_model = 'awayscore_ols_2002_2020.pickle'\n",
    "\n",
    "# Save files to model folder\n",
    "hres.save(join(model_path, home_model))\n",
    "ares.save(join(model_path, away_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### PREDICTIONS, TABLES, & PLOTTING ###\n",
    "#######################################\n",
    "\n",
    "## 2021 DATA \n",
    "\n",
    "# Predict Scores\n",
    "home_score = predict_score(X_testTrans, Xr_hols, Xr_hvif, Xr_hols1, hres)\n",
    "away_score = predict_score(X_testTrans, Xr_aols, Xr_avif, Xr_aols1, ares)\n",
    "print(list(zip(home_score,away_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "### MODELLING: SK LINEAR REGRESSION ###\n",
    "#######################################\n",
    "\n",
    "# Numerical cols to StandardScaler\n",
    "num_cols = [col for col in list(X_train.select_dtypes\\\n",
    "                                         (include=['float64', 'int64']).columns)\\\n",
    "                    if 'stat' in col or 'record' in col]\n",
    "\n",
    "# Categorical cols to OHE\n",
    "cat_cols = ['division_home', 'conference_home', \n",
    "            'location_home', 'team_name_home',\n",
    "            'division_away', 'conference_away', \n",
    "            'location_away', 'team_name_away']\n",
    "\n",
    "# Run Model\n",
    "linreg_model = pipe_grid_model(X_train, y_train,\n",
    "                               num_cols, cat_cols,\n",
    "                               estimator='LinearRegression')\n",
    "\n",
    "print(linreg_model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49e9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fff233",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "### MODELLING: Sk SVR REGRESSION ###\n",
    "####################################\n",
    "\n",
    "# Numerical cols to StandardScaler\n",
    "num_cols = [col for col in list(X_train.select_dtypes\\\n",
    "                                         (include=['float64', 'int64']).columns)\\\n",
    "                    if 'stat' in col or 'record' in col]\n",
    "\n",
    "# Categorical cols to OHE\n",
    "cat_cols = ['division_home', 'conference_home', \n",
    "            'location_home', 'team_name_home',\n",
    "            'division_away', 'conference_away', \n",
    "            'location_away', 'team_name_away']\n",
    "\n",
    "# Run Model\n",
    "svr_model = pipe_grid_model(X_train, y_train,\n",
    "                            num_cols, cat_cols,\n",
    "                            estimator='SVR')\n",
    "\n",
    "print(svr_model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839587a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "### SAVE MODELS ###\n",
    "###################\n",
    "\n",
    "# Model path\n",
    "model_path = '../model'\n",
    "\n",
    "# file names\n",
    "svr_modelName = 'scores_svr_2002_2020.joblib'\n",
    "\n",
    "# Save files to model folder\n",
    "dump(svr_model, join(model_path, svr_modelName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e15ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80143a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
